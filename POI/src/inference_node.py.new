"""Core inference node implementation."""

import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
import torch
from web3 import Web3
from web3.types import Address

from .gpu_memory_manager import GPUMemoryManager
from .model_predictor import ModelPredictor
from .location_manager import LocationManager

class InferenceNode:
    def __init__(
        self,
        contract_address: Address,
        account: Account,
        web3: Web3,
        models_dir: str = "./models",
        gpu_buffer: float = 0.4,
        cache_size: int = 5
    ):
        self.models_dir = models_dir
        self.account = account
        self.web3 = web3
        
        # Initialize optimized components
        self.gpu_manager = GPUMemoryManager(buffer_ratio=gpu_buffer)
        self.model_predictor = ModelPredictor(cache_size=cache_size)
        self.location_manager = LocationManager()
        
        # Register node location (should be configured)
        self.latitude = 0.0  # Default values
        self.longitude = 0.0
        self.location_manager.add_node(
            str(self.account.address),
            self.latitude,
            self.longitude,
            gpu_capacity=torch.cuda.device_count() if torch.cuda.is_available() else 1
        )
        
        self._load_contract(contract_address)

    def _load_contract(self, address: Address):
        """Load the marketplace contract."""
        contract_path = Path(__file__).parent / "contracts"
        abi_file = contract_path / "ModelMarketplaceV2.json"
        
        with open(abi_file) as f:
            contract_json = json.load(f)
            self.contract = self.web3.eth.contract(
                address=address,
                abi=contract_json["abi"]
            )
            
    def _preload_model(self, model_id: str) -> None:
        """Preemptively load a model into GPU memory."""
        try:
            # Check if model should be cached based on usage patterns
            if not self.model_predictor.should_cache_model(model_id):
                return
                
            # Get model path and load model file
            model_path = Path(self.models_dir) / model_id
            if not model_path.exists():
                logging.warning(f"Model {model_id} not found locally")
                return
                
            # Load model into CPU memory first
            model = torch.load(model_path / "model.pt", map_location="cpu")
            
            # Try to move to GPU if we have enough memory
            if torch.cuda.is_available():
                # Estimate model size (rough estimation)
                model_size = sum(p.numel() * p.element_size() for p in model.parameters())
                
                if self.gpu_manager.can_load_model(model_id, model_size):
                    self.gpu_manager.cache_model(model_id, model)
                    logging.info(f"Preloaded model {model_id} to GPU cache")
                    
        except Exception as e:
            logging.error(f"Failed to preload model {model_id}: {e}")
            
    def _load_model_to_gpu(self, model_id: str) -> torch.nn.Module:
        """Load a model and move it to GPU."""
        model_path = Path(self.models_dir) / model_id
        if not model_path.exists():
            raise ValueError(f"Model {model_id} not found")
            
        # Load model to CPU first
        model = torch.load(model_path / "model.pt", map_location="cpu")
        
        # Move to GPU if available
        if torch.cuda.is_available():
            # Ensure we have enough memory
            self.gpu_manager.optimize_memory(1024*1024*1024)  # Request 1GB buffer
            self.gpu_manager.cache_model(model_id, model)
            
        return model
        
    def execute_inference(self, model_id: str, input_data: Any, user_id: Optional[str] = None) -> Dict[str, Any]:
        """Execute model inference with optimized GPU usage and generate proof."""
        start_time = time.time()
        
        # Update usage patterns if user_id provided
        if user_id:
            self.model_predictor.record_usage(user_id, model_id)
            next_models = self.model_predictor.predict_next_models(user_id)
            # Pre-emptively load predicted models
            for next_model in next_models:
                if next_model not in self.gpu_manager.cached_models:
                    self._preload_model(next_model)
        
        # Get model from GPU cache or load it
        model = self.gpu_manager.get_cached_model(model_id)
        if not model:
            model = self._load_model_to_gpu(model_id)
        
        with torch.no_grad():
            # Convert input to tensor efficiently
            if not isinstance(input_data, torch.Tensor):
                input_data = torch.tensor(input_data, device='cuda' if torch.cuda.is_available() else 'cpu')
            elif torch.cuda.is_available() and not input_data.is_cuda:
                input_data = input_data.cuda()
            
            # Run inference with error handling
            try:
                output_data = model(input_data)
            except RuntimeError as e:
                # Handle OOM errors by freeing memory and retrying
                if "out of memory" in str(e):
                    self.gpu_manager.optimize_memory(required_memory=1024*1024*1024)  # Request 1GB
                    output_data = model(input_data)
                else:
                    raise
            
            # Generate proof asynchronously if possible
            proof = self.generate_proof(model_id, input_data, output_data)
            
            # Get hashes efficiently
            input_hash = self._hash_data(input_data)
            output_hash = self._hash_data(output_data)
            
            # Submit proof with potential batching
            execution_id = self.submit_proof(
                model_id,
                input_hash,
                output_hash,
                proof
            )
            
            # Update node load for location manager
            if torch.cuda.is_available():
                current_load = torch.cuda.memory_allocated(0) / torch.cuda.get_device_properties(0).total_memory
                self.location_manager.update_node_load(str(self.account.address), int(current_load * 100))
            
            end_time = time.time()
            inference_time = int((end_time - start_time) * 1000)  # Convert to milliseconds
            
            return {
                'output': output_data.cpu().numpy(),
                'executionId': execution_id,
                'proof': proof.hex(),
                'latency': {'inference_ms': inference_time}
            }